# ğŸ§  Deep Q-Learning Agent for Atari: Kung Fu Master

**Course:** Reinforcement Learning and AI Agents  
**Assignment:** LLM Agents & Deep Q-Learning with Atari Games  
**Prepared By:** Ritwik Giri  
**Submission Date:** November 9, 2025

* * *

## ğŸ¯ Project Overview

This project implements a **Deep Q-Learning (DQN)** agent in the **Atari Kung Fu Master** environment using **OpenAI Gymnasium**.  
The goal is to train an agent to maximize in-game score by learning optimal policies through **value-based reinforcement learning**.

The assignment explores how agents learn via **interaction and feedback**, emphasizing:

*   Environment explorationâ€“exploitation balance
*   Bellman equation parameter sensitivity
*   Reward shaping and target network stability
*   Theoretical connections to **LLM-based reinforcement** (RLHF)

* * *

## âš™ï¸ Environment Setup

### 1ï¸âƒ£ Create and Activate Virtual Environment

    # Create virtual environment
    python3 -m venv .venv
    
    # Activate environment (Linux/Mac)
    source .venv/bin/activate
    
    # Activate environment (Windows)
    # .venv\Scripts\activate
    

### 2ï¸âƒ£ Install Dependencies

    # Upgrade pip
    pip install --upgrade pip
    
    # Install required packages
    pip install "gymnasium[atari,accept-rom-license]" ale-py
    pip install torch torchvision numpy opencv-python matplotlib pandas tqdm
    

### 3ï¸âƒ£ Verify Environment

    import gymnasium as gym
    
    # Create environment
    env = gym.make("ALE/KungFuMaster-v5")
    obs, info = env.reset()
    
    # Check observation and action spaces
    print(f"Observation shape: {obs.shape}")
    print(f"Action space: {env.action_space}")
    

**Expected Output:**

    Observation shape: (210, 160, 3)
    Action space: Discrete(14)
    

* * *

## ğŸ“ Project Structure

    dqn_kungfu_master/
    â”‚
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ train_dqn.py           # Training loop with hyperparameter tuning
    â”‚   â”œâ”€â”€ evaluate_dqn.py        # Evaluation + video recording
    â”‚   â”œâ”€â”€ replay_buffer.py       # Experience replay implementation
    â”‚   â”œâ”€â”€ q_network.py           # PyTorch CNN-based Q-network
    â”‚   â”œâ”€â”€ utils.py               # Logging, metrics, and plotting helpers
    â”‚   â”œâ”€â”€ wrappers.py            # Preprocessing: grayscale, frame stack
    â”‚
    â”œâ”€â”€ notebooks/
    â”‚   â””â”€â”€ DeepQLearning_KungFuMaster.ipynb  # Analysis + plots + Q&A
    â”‚
    â”œâ”€â”€ outputs/
    â”‚   â”œâ”€â”€ metrics_baseline.csv
    â”‚   â”œâ”€â”€ metrics_gamma095_lr1e4.csv
    â”‚   â”œâ”€â”€ metrics_epsdecay25k.csv
    â”‚   â”œâ”€â”€ metrics_policy_softmax.csv
    â”‚   â”œâ”€â”€ metrics_reward_clipped.csv
    â”‚   â”œâ”€â”€ metrics_tsync_10k.csv
    â”‚   â”œâ”€â”€ trained_model.pth
    â”‚   â”œâ”€â”€ demo_trained.mp4
    â”‚   â”œâ”€â”€ demo_baseline.mp4
    â”‚   â”œâ”€â”€ comparison_baseline_vs_trained.mp4
    â”‚
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ README.md
    
    

* * *

## ğŸš€ Quick Start

### 1ï¸âƒ£ Train the Agent

    python -m src.train_dqn --episodes 100 --gamma 0.95 --lr 1e-4 --target_sync 10000
    

### 2ï¸âƒ£ Evaluate and Record Gameplay

    python -m src.evaluate_dqn --weights outputs/trained_model.pth --episodes 5 --record outputs/demo_trained.mp4
    

* * *

## ğŸ“Š Key Experiments

| Experiment | Configuration | Result |
| --- | --- | --- |
| Bellman parameters | Î³=0.95, lr=1eâˆ’4 | Stable learning |
| Exploration tuning | Faster Îµ-decay | Improved convergence |
| Reward clipping | [-1, 1] range | Reduced variance |
| Target sync | 10k steps | Balanced update frequency |

* * *

## ğŸ§  Project Highlights

*   Built using **PyTorch + Gymnasium + ALE-py**
*   End-to-end DQN training and evaluation pipeline
*   Includes gameplay recordings and experiment logs
*   Links classical RL to LLM-based reinforcement concepts (RLHF)

* * *

## âš–ï¸ Code Attribution

### âœ… Original Code

*   **`train_dqn.py`** â€” Main training logic, epsilon scheduling, reward clipping, and target sync
*   **`evaluate_dqn.py`** â€” Evaluation, greedy policy toggle, and MP4 recording
*   **`replay_buffer.py`** â€” Replay memory built from scratch
*   **`q_network.py`** â€” PyTorch CNN architecture tailored for Kung Fu Master
*   **`utils.py`** â€” Metrics, plotting, CSV logging
*   **Notebook** â€” All parameter sweeps, plots, and analysis

### ğŸ§© Adapted & Referenced Sources

*   [OpenAI Gymnasium](https://gymnasium.farama.org/) (environment and wrapper templates)
*   [PyTorch DQN Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) (initial structure for Q-network updates)
*   [ALE-py](https://github.com/mgbellemare/Arcade-Learning-Environment) (Arcade Learning Environment backend)

* * *

## ğŸ Conclusion

This project demonstrates a successful end-to-end implementation of a Deep Q-Learning agent for Atari's Kung Fu Master. Through iterative experimentation and parameter optimization, the agent displayed measurable learning stability consistent with core RL theory. The study also bridges the conceptual gap between traditional reinforcement learning and modern LLM optimization, reinforcing how reward-driven learning generalizes across AI paradigms.


* * *

## ğŸ‘ Acknowledgments

Special thanks to:

*   Professor & TA team for providing clear assignment rubrics and evaluation structure
*   OpenAI Gymnasium & PyTorch for open educational resources

* * *

## ğŸ“„ License

This project was developed as part of academic coursework for educational purposes.

* * *

<p align="center"> <strong>ğŸ® Built with Deep Reinforcement Learning ğŸ¤–</strong> </p>
